{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da1ac4-11cf-45c1-a07c-21aa9b840c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Installing dependencies\n",
    "!pip install trl\n",
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f2965b-adec-4fe2-a946-52091c527494",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up kaggle environment variables\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import huggingface_hub\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "huggingface_hub.login(secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6734dc-aa80-4794-a5b9-2721195e5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing dependencies\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5a0d4-74d5-4303-a499-80b720844aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up model and dataset variables\n",
    "model = \"Qwen/Qwen2.5-3B\"\n",
    "dataset = \"sharmaarush/Pytorch_QA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5667a348-ce95-46c2-a28b-6595f9a15c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset helper functions (including chat templates, tokenization)\n",
    "from functools import partial\n",
    "\n",
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Format QA pairs for PyTorch QA training\n",
    "    :param sample: Sample dictionary with keys ['user_q', 'question', 'user_a', 'answer']\n",
    "    \"\"\"\n",
    "    INTRO_BLURB = \"Below is a question asked by a user. Write the most helpful and accurate answer.\"\n",
    "    QUESTION_KEY = \"### Question:\"\n",
    "    ANSWER_KEY = \"### Answer:\"\n",
    "    END_KEY = \"### End\"\n",
    "\n",
    "    blurb = f\"\\n{INTRO_BLURB}\"\n",
    "    user_question = f\"Asked by {sample['user_q']}:\\n{QUESTION_KEY}\\n{sample['question']}\"\n",
    "    user_answer = f\"Answered by {sample['user_a']}:\\n{ANSWER_KEY}\\n{sample['answer']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "\n",
    "    # Collect non-empty parts\n",
    "    parts = [part for part in [blurb, user_question, user_answer, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
    "\n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=['user_q', 'question', 'user_a', 'answer'],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6badb62-c0c5-419e-a2d5-7517271f793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset pre processing\n",
    "dataset = load_dataset(dataset)\n",
    "train_test_split = dataset[\"train\"].train_test_split(test_size=0.3, seed=42)\n",
    "test_validation_split = train_test_split[\"test\"].train_test_split(test_size=1/3, seed=42)\n",
    "\n",
    "dataset_train = train_test_split[\"train\"]\n",
    "dataset_validation = test_validation_split[\"train\"]\n",
    "dataset_test = test_validation_split[\"test\"]\n",
    "\n",
    "print(\"training dataset \", len(dataset_train))\n",
    "print(\"validation dataset \", len(dataset_validation))\n",
    "print(\"test dataset \", len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b80f3-9927-48fe-8660-43f749a870a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading model, bnb config and tokenizer\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "base_model = model\n",
    "device_map = \"auto\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map=device_map,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True, padding_side=\"left\", add_eos_token=True, add_bos_token=True, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dbd632-5cb8-4440-a1dc-566be424e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up training params\n",
    "# Training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense'\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable() ## For effecient memmory usage (but is slow)\n",
    "\n",
    "peft_model = get_peft_model(model, config)\n",
    "\n",
    "output_dir = \"content/model\"\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    warmup_steps=1,\n",
    "    per_device_train_batch_size=2,     # Adjust according to GPU memory\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    fp16=True,                         # Added this - crucial for memory\n",
    "    logging_steps=25,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    # save_steps=50,                     # Reduce checkpoint frequency\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    do_eval=True,\n",
    "    per_device_eval_batch_size=1,\n",
    "    dataloader_num_workers=0,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    overwrite_output_dir=True,\n",
    "    group_by_length=True,\n",
    "    dataloader_pin_memory=False,\n",
    "    max_grad_norm=0.3,\n",
    ")\n",
    "\n",
    "peft_model.config.use_cache = False\n",
    "peft_trainer = transformers.Trainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=dataset_train_pre,\n",
    "    eval_dataset=dataset_validation_pre,\n",
    "    args=peft_training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c0dec0-f1e0-46dd-a28d-3ba5387a8a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the model\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d24446d-34b6-4cfb-9e08-376c9944971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inferencing the model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sharmaarush/pytorch_QA_model\")\n",
    "\n",
    "# Load base model (same one you fine-tuned on)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load LoRA adapter on top\n",
    "model = PeftModel.from_pretrained(base_model, \"sharmaarush/pytorch_QA_model\")\n",
    "\n",
    "# Create pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ba1af-e382-4919-8b36-fce7e997fe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "prompt = \"Explain FSDP2 in easier terms\"\n",
    "outputs = pipe(prompt, do_sample=True, temperature=0.7)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
